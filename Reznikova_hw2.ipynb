{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №3: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr\\>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 8 апреля 2019, 06:00 <br\\>\n",
    "**Штраф за опоздание:** -2 балла после 06:00 8 апреля, -4 балла после 06:00 15 апреля, -6 баллов после 06:00 22 апреля  -8 баллов после 06:00 29 апреля.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла<br\\>\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Что почитать по теории ***\n",
    "\n",
    "Одна из лучших книг по ML $-$ \"Pattern Recognition and Machine Learning\" Bishop, Christopher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "$$Q_r(w) =  Q(w) + \\frac{1}{C}R(w) $$\n",
    "\n",
    "$$Q_r(w) =  \\frac{1}{N}\\left[\\sum_i L(w) \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для мини-батч градиентного спуска размера n считаем градиент:\n",
    "$$\\nabla_w Q_r^*(w_{old}) = \\frac{1}{n}\\nabla_w\\left[\\sum_{j=1}^{n} L(w_{old}, x_j, y_j) \\right] + \\frac{1}{C}\\nabla_w R(w_{old}) $$\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha \\nabla_w Q_r^*(w_{old}) $$\n",
    "\n",
    "$$\\nabla_w Q_r^*(w_{old}) = \\frac{1}{n}\\left[\\sum_{j=1}^{n} \\nabla_w L(w_{old}, x_j, y_j) \\right] + \\frac{2}{C}w_{old} $$\n",
    "\n",
    "\n",
    "$$ w_{new} = w_{old} - \\frac{\\alpha}{n}\\left[\\sum_{j=1}^{n} \\nabla_w L(w_{old}, x_j, y_j) \\right] - \\frac{2\\alpha}{C}w_{old} $$\n",
    "\n",
    "$$ w_{new} = w_{old}( 1 - \\frac{2\\alpha}{C} ) - \\frac{\\alpha}{n}\\left[\\sum_{j=1}^{n} \\nabla_w L(w_{old}, x_j, y_j) \\right] $$\n",
    "\n",
    "если функция потерь задана $L(w) = ( y_i - a_i ) ^ 2 $,\n",
    "где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$,\n",
    "соответственно, функционал потерь $Q(w) =  \\frac{1}{N}\\left[\\sum_i ( y_i - a_i ) ^ 2 \\right]$, то градиент:\n",
    "\n",
    "$$\\nabla_w Q_r^*(w_{old}) = \\frac{2}{n}\\left[\\sum_{j=1}^{n}(\\langle\\,x_j,w_{old}\\rangle - y_j ) x_j \\right] + \\frac{2}{C}w_{old} $$\n",
    "\n",
    "\n",
    "\n",
    "веса обновляются:\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha( \\frac{2}{n}\\left[\\sum_{j=1}^{n}( \\langle\\,x_j,w_{old}\\rangle - y_j ) x_j \\right] +\n",
    "\\frac{2}{C} w_{old} ) $$\n",
    "\n",
    "$$ w_{new} = w_{old}( 1 - \\frac{2\\alpha}{C} ) - \\frac{2\\alpha}{n}\\left[\\sum_{j=1}^{n}( \\langle\\,x_j,w_{old}\\rangle - y_j ) x_j \\right] $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha \\nabla_w Q_r^*(w_{old}) $$\n",
    "\n",
    "Для мини-батч градиентного спуска размера n считаем градиент:\n",
    "$$\\nabla_w Q_r^*(w_{old}) = \\frac{1}{n}\\nabla_w\\left[\\sum_{j=1}^{n} L(w_{old}, x_j, y_j) \\right] + \\frac{1}{C}\\nabla_w R(w_{old}) $$\n",
    "\n",
    "$$L(w) =  -( y_i \\log a_i + ( 1 - y_i ) \\log ( 1 - a_i ) ) $$,\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$, $a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ) = \\frac{1}{1 + \\exp (-\\langle\\,x_i,w\\rangle)}$\n",
    "\n",
    "$$\\nabla_w L(w) = -( y_i \\frac{1}{a_i} - ( 1 - y_i ) \\frac{1}{1 - a_i})\\nabla_w a_i = -( y_i ( 1 - a_i ) - a_i ( 1 - y_i ) )\\frac{1}{a_i ( 1 - a_i )} ( -a_i ) ( 1 - a_i ) ( -x_i) = ( a_i - y_i ) x_i $$\n",
    "\n",
    "\n",
    "$$\\nabla_w Q_r^*(w_{old}) = \\frac{1}{n}\\left[\\sum_{j=1}^{n}( \\sigma( \\langle\\,x_i,w\\rangle ) - y_j ) x_j \\right] + \\frac{2}{C}w_{old} $$\n",
    "\n",
    "\n",
    "\n",
    "веса обновляются:\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha(\\frac{1}{n}\\left[\\sum_{j=1}^{n}( \\sigma( \\langle\\,x_i,w\\rangle ) - y_j ) x_j \\right] + \\frac{2}{C}w_{old}) $$\n",
    "\n",
    "$$ w_{new} = w_{old}(1 - \\frac{2\\alpha}{C}) - \\frac{\\alpha}{n}\\left[\\sum_{j=1}^{n}(\\sigma( \\langle\\,x_i,w\\rangle ) - y_j ) x_j \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    n_ind = y.shape[0]\n",
    "    while True:\n",
    "        ind_ = np.arange(y.shape[0])\n",
    "        X_batch = X[:n_ind, :]\n",
    "        y_batch = y[:n_ind]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(ind_)\n",
    "            ind = ind_[: batch_size]\n",
    "            X_batch = X[ind, :]\n",
    "            y_batch = y[ind]\n",
    "        yield (X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%pycodestyle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "\n",
    "    sigm_value_x = 1. / (1. + np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, batch_generator, batch_size, C=1,\n",
    "                 alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "\n",
    "        self.C = 1 / C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.batch_size = batch_size\n",
    "        self.errors_log = {'iter': [], 'loss': []}\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def calc_dot(self, X_batch):\n",
    "        \"\"\"\n",
    "        Считаем  скалярное произведение\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        \"\"\"\n",
    "\n",
    "        if X_batch.shape[1] != self.weights.shape[0]:\n",
    "            y_dot = np.dot(np.c_[X_batch, np.ones(X_batch.shape[0])],\n",
    "                           self.weights.T)\n",
    "        else:\n",
    "            y_dot = np.dot(X_batch, self.weights.T)\n",
    "        if self.model_type == 'log_reg':\n",
    "            y_dot = sigmoid(y_dot)\n",
    "        return y_dot\n",
    "\n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = self.calc_dot(X_batch)\n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss = np.mean(np.square(y_batch - y_hat))\n",
    "        elif self.model_type == 'log_reg':\n",
    "            loss = np.mean(- y_batch * np.log(y_hat) -\n",
    "                             (1. - y_batch) * np.log(1. - y_hat))\n",
    "        L2 = self.C * np.dot(self.weights, self.weights.T)\n",
    "        loss += L2\n",
    "        return loss\n",
    "\n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = self.calc_dot(X_batch)\n",
    "        err = np.expand_dims((y_hat - y_batch), axis=1)\n",
    "        loss_grad = np.sum(err * X_batch, axis=0)\n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss_grad *= 2\n",
    "        L2_grad = 2 * self.C * self.weights\n",
    "        loss_grad += L2_grad\n",
    "        return loss_grad\n",
    "\n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "\n",
    "        self.weights -= self.alpha * new_grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        self.weights = np.random.rand(X.shape[1] + 1)\n",
    "        self.steps_per_epoch = math.ceil(X.shape[0] / batch_size)\n",
    "        step = 100\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator =\\\n",
    "                self.batch_generator(np.c_[X, np.ones(X.shape[0])], y,\n",
    "                                     shuffle=True, batch_size=self.batch_size)\n",
    "            batch_loss_aver = 0\n",
    "            for batch_num in range(self.steps_per_epoch):\n",
    "                new_batch = next(new_epoch_generator)\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss_aver += self.calc_loss(X_batch, y_batch)\n",
    "                if (batch_num + 1) % step == 0:\n",
    "                    self.errors_log['iter'].append(batch_num + 1 / step)\n",
    "                    self.errors_log['loss'].append(batch_loss_aver / step)\n",
    "                    batch_loss_aver = 0\n",
    "#         print('aver_loss: ', self.errors_log['loss'])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        y_hat = self.calc_dot(X)\n",
    "        if self.model_type == 'log_reg':\n",
    "            ind = (y_hat > 0.5).astype(np.int)\n",
    "            y_hat = self.classes[ind]\n",
    "        return y_hat.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf):\n",
    "    x = np.linspace(-5, 8)\n",
    "    w = clf.weights\n",
    "    print(\"{0} weights: {1}\".format(clf.model_type, w))\n",
    "# w[0] * x_0 + w[1]* x_1 + w[-1] = 1/2\n",
    "    yy = (0.5 - w[0] * x - w[-1]) / w[1]\n",
    "    plt.plot(x, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_reg weights: [ 0.17710001 -0.13092685  0.02892726]\n",
      "log_reg weights: [ 0.15936762 -0.00878866 -0.05328208]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xa22e7b438>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.data import iris_data\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "my_lin_reg = MySGDClassifier(batch_generator, batch_size, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg')\n",
    "my_lin_reg.fit(X, y)\n",
    "plot_decision_boundary(my_lin_reg)\n",
    "\n",
    "\n",
    "my_log_reg = MySGDClassifier(batch_generator, batch_size, C=1, alpha=0.01, max_epoch=10, model_type='log_reg')\n",
    "my_log_reg.fit(X, y)\n",
    "plot_decision_boundary(my_log_reg)\n",
    "# print('my_predict: ', my_log_reg.predict(X))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100000, n_features=10, \n",
    "                           n_informative=4, n_redundant=0, \n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что Вы можете сказать про сходимость метода при различных `alpha`? Какое значение стоит выбирать для лучшей сходимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (3  балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview-2.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна. Если да, то Вы молодец!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контрольные вопросы\n",
    "Постарайтесь максимально развернуто и честно ответить на вопросы. Они охватывают тему линейных моделей и скорее нужны преподавателям, чтобы понимать, что именно Вы усвоили плохо. Надеюсь, они подскажут, что именно в теме Вы не понимаете или наоборот порадают, что Вы все знаете ^_^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Опишите основные, на Ваш взгляд,  отличия логистической регрессии от линейной регрессии. Почему, на ваш взгляд, задачу классификации решают логистической, а не линейной регрессией?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Как Вы думаете, для каких типов задач (объем данных, число признаков, типы признаков) стоит отдавать предпочтение линейным моделям?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Пусть на обучении мы имеем выборку размера $N$, число признаков $D$. Чему равна алгоритмическая сложность одного шага градиентного спуска? Cтохастического градиентного спуска?  Сложность предсказания на одном объекте?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В чем преимущества стохастического градиентного спуска (в том числе мини-батч) над обычным градиентным спуском? В чем его недостатки? Рассмотрите несколько аспектов $-$ скорость сходимости, необходимость загрузки всех данных в оперативную память, сложность вычисления одного шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Как Вы думаете, при обучении линейной модели с помощью SGD, ошибку на новом объекте стоит считать до итерации спуска на этом объекте или после? Почему Вы так думаете? Возможно, Вам будет интересно ознакомиться с http://hunch.net/~jl/projects/prediction_bounds/thesis/mathml/thesisse44.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Как Вы думаете, во времена такого бума нейронных сетей, остаются ли популярными линейные модели, или это уже пережиток прошлого? Почему Вы так думаете?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
